{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8225\n",
    "\n",
    "\n",
    "## TP4 Hiver 2020 - Analyse de sentiments sur des reviews de films\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - Anhoury, Marc (1846809) \n",
    "    - Brisebois-Roy, Antoine (1846113) \n",
    "    - Arseaneault, Samuel (1850861)\n",
    "    - Toukal, Monssaf (1850319)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Installation\n",
    "\n",
    "Nous avons besoin des librairies `numpy`, `sklearn` et `scipy`, ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ImMay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ImMay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ImMay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\ImMay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# !pip install --user numpy\n",
    "# !pip install --user sklearn\n",
    "# !pip install --user scipy\n",
    "# !pip install --user nltk\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"universal_tagset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jeu de données\n",
    "\n",
    "On utilise un jeu de donnée provenant de Stanford.edu: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Pour citer la source originale de la base :\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "Les données dans cette base de données sont séparées en ensemble d'entrainement et en ensemble de test. En tout, le dataset contient 50 000 reviews de film trouvées sur IMDB, dont 25 000 positives et 25 000 négatives. Sur IMDB, les utilisateurs donnent une note sur 10 aux films. Dans le dataset, les films ayant obtenus un score inférieur ou égal à 4 sont classés comme étant négatifs tandis que les films ayant obtenus un score supérieur ou égal à 7 sont classés comme étant positif. \n",
    "\n",
    "Dans le dataset, il peut y avoir un maximum de 30 reviews pour un même film.\n",
    "\n",
    "Pour les besoins de ce TP, nous avons extrait tous les reviews des fichiers .txt dans un fichier .csv ayant comme première colonne le texte du review et comme deuxième colonne le sentiment (0 pour négatif, 1 pour positif). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def create_csv_from_txts(path):\n",
    "        with open('movies_review.csv', 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['Review', 'Sentiment'])\n",
    "            for cat in ['neg', 'pos']:\n",
    "                for file in glob.glob(f\"{path}/{cat}/*.txt\"):\n",
    "                    with open(file, mode=\"r\", encoding=\"utf-8\") as fd:\n",
    "                        infos = []\n",
    "                        content = fd.read()\n",
    "                        content = content.strip(';')\n",
    "                        infos.append(content)\n",
    "                        infos.append(0 if cat == 'neg' else 1)\n",
    "                        writer.writerow(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_csv_from_txts('data') # RUN ONLY ONCE IF NO CSV IN DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ensuite chargé les données en un ensemble d'entrainement, un ensemble de validation et un ensemble de test. De plus, dès le chargement, nous avons pensé enlever les tags HTML des textes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set :  34850\n",
      "Length of validation set :  7650\n",
      "Length of test set :  7500\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"latin-1\") as csvfile:\n",
    "        \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        # Taking the header of the file + the index of useful columns:\n",
    "        header = next(reader)\n",
    "        cleanr = re.compile('<.*?>')\n",
    "\n",
    "        for row in reader:\n",
    "            x.append(re.sub(cleanr, '', row[0]))\n",
    "            label = row[1]\n",
    "            y.append(label)\n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Path of the dataset\n",
    "path = \"data/movies_reviews.csv\"\n",
    "\n",
    "X, y = load_dataset(path)\n",
    "\n",
    "y = [int(value) for value in y ]\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement\n",
    "\n",
    "Nous avons implémenter la *tokenization* et le *stemming*, qui sont 2 étapes courantes de preprocessing en NLP.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
    "\n",
    "Par exemple la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
    "\n",
    "De plus, dans notre code, nous avons pris l'initiative d'enlever du tableau de *tokens* tous les signes de ponctuations pour ne pas avoir d'information supperflue.\n",
    "\n",
    "**De plus, tous les tokenizers ont également le rôle de mettre le texte en minuscule.**\n",
    "\n",
    "\n",
    "- Le **NLTKTokenizer**, qui utilise la méthode du package *nltk* (https://www.nltk.org/api/nltk.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "\n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        review_tknzr = TweetTokenizer()\n",
    "        # Have to return a list of tokens\n",
    "        tokens = review_tknzr.tokenize(text)\n",
    "        tokens = [''.join(c for c in s if c not in string.punctuation) for s in tokens]\n",
    "        tokens = [s for s in tokens if s]\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Im', 'warning', 'you', 'people', 'out', 'there', 'this', 'is', 'just', 'a', 'waste', 'of', 'your', 'time', 'I', 'am', 'being', 'honest', 'when', 'Im', 'saying', 'that', 'this', 'is', 'the', 'worst', 'movie', 'Ive', 'ever', 'seen', 'Its', 'just', 'a', 'move', 'about', 'Christian', 'propaganda', 'Dont', 'throw', 'away', 'your', 'life', 'dont', 'see', 'it', 'I', 'think', 'they', 'made', 'the', 'movie', 'so', 'more', 'people', 'will', 'believe', 'in', 'Jesus', 'or', 'something', 'but', 'it', 'works', 'in', 'the', 'opposite', 'way', 'The', 'actors', 'are', 'all', 'newbies', 'the', 'story', 'is', 'just', 'fuzzy', 'I', 'think', 'this', 'movie', 'is', 'a', 'work', 'of', 'the', 'devil', 'This', 'movie', 'is', 'just', 'not', 'worth', 'seeing', 'so', 'please', 'take', 'my', 'advice', 'and', 'dont']\n"
     ]
    }
   ],
   "source": [
    "nltk_tknzr = NLTKTokenizer()\n",
    "test = train_X[1]\n",
    "nltk_tokens = nltk_tknzr.tokenize(test)\n",
    "print(nltk_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troncature (ou Stemming)\n",
    "\n",
    "Permet de conserver seulement la racine des mots sans \"overhead\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, token):\n",
    "        \"\"\"\n",
    "        token: a string that contain a token\n",
    "        \"\"\"\n",
    "        stem_token = self.stemmer.stem(token)\n",
    "        return stem_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im  :  im\n",
      "warning  :  warn\n",
      "you  :  you\n",
      "people  :  peopl\n",
      "out  :  out\n",
      "there  :  there\n",
      "this  :  this\n",
      "is  :  is\n",
      "just  :  just\n",
      "a  :  a\n",
      "waste  :  wast\n",
      "of  :  of\n",
      "your  :  your\n",
      "time  :  time\n",
      "I  :  i\n",
      "am  :  am\n",
      "being  :  being\n",
      "honest  :  honest\n",
      "when  :  when\n",
      "Im  :  im\n",
      "saying  :  say\n",
      "that  :  that\n",
      "this  :  this\n",
      "is  :  is\n",
      "the  :  the\n",
      "worst  :  worst\n",
      "movie  :  movi\n",
      "Ive  :  ive\n",
      "ever  :  ever\n",
      "seen  :  seen\n",
      "Its  :  its\n",
      "just  :  just\n",
      "a  :  a\n",
      "move  :  move\n",
      "about  :  about\n",
      "Christian  :  christian\n",
      "propaganda  :  propaganda\n",
      "Dont  :  dont\n",
      "throw  :  throw\n",
      "away  :  away\n",
      "your  :  your\n",
      "life  :  life\n",
      "dont  :  dont\n",
      "see  :  see\n",
      "it  :  it\n",
      "I  :  i\n",
      "think  :  think\n",
      "they  :  they\n",
      "made  :  made\n",
      "the  :  the\n",
      "movie  :  movi\n",
      "so  :  so\n",
      "more  :  more\n",
      "people  :  peopl\n",
      "will  :  will\n",
      "believe  :  believ\n",
      "in  :  in\n",
      "Jesus  :  jesus\n",
      "or  :  or\n",
      "something  :  someth\n",
      "but  :  but\n",
      "it  :  it\n",
      "works  :  work\n",
      "in  :  in\n",
      "the  :  the\n",
      "opposite  :  opposit\n",
      "way  :  way\n",
      "The  :  the\n",
      "actors  :  actor\n",
      "are  :  are\n",
      "all  :  all\n",
      "newbies  :  newbi\n",
      "the  :  the\n",
      "story  :  stori\n",
      "is  :  is\n",
      "just  :  just\n",
      "fuzzy  :  fuzzi\n",
      "I  :  i\n",
      "think  :  think\n",
      "this  :  this\n",
      "movie  :  movi\n",
      "is  :  is\n",
      "a  :  a\n",
      "work  :  work\n",
      "of  :  of\n",
      "the  :  the\n",
      "devil  :  devil\n",
      "This  :  this\n",
      "movie  :  movi\n",
      "is  :  is\n",
      "just  :  just\n",
      "not  :  not\n",
      "worth  :  worth\n",
      "seeing  :  see\n",
      "so  :  so\n",
      "please  :  pleas\n",
      "take  :  take\n",
      "my  :  my\n",
      "advice  :  advic\n",
      "and  :  and\n",
      "dont  :  dont\n"
     ]
    }
   ],
   "source": [
    "snow_stemmer = Stemmer()\n",
    "\n",
    "for token in nltk_tokens:\n",
    "    print(token, \" : \", snow_stemmer.stem(token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Un pipeline permet d'exécuter séquentiellement toutes les étapes de preprocessing, pour transformer les données brutes en une version utilisable pour notre modèle. La *PreprocessingPipeline* a été implémenter pour appliquer à la suite le tokenizer et les troncatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_emoji = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "sad_emoji = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "def replace_emoji(token):\n",
    "    if token in happy_emoji:\n",
    "        return \":)\"\n",
    "    if token in sad_emoji:\n",
    "        return \":(\"\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPreprocessing(object):\n",
    "\n",
    "    def preprocess(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: original list of tokens\n",
    "        \"\"\"\n",
    "        removed_chars = [\"http\", \"@\"] # Utilisation de list si jamais on veux ajouter des conditions\n",
    "        new_tokens = [replace_emoji(token) for token in tokens if not any(character in token.lower() for character in removed_chars)]\n",
    "\n",
    "        # return the preprocessed twitter\n",
    "        return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "\n",
    "    def __init__(self, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        emojiPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = NLTKTokenizer()\n",
    "        self.linkPreprocesser = LinkPreprocessing()\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "\n",
    "    def preprocess(self, review):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(review)\n",
    "\n",
    "        if self.stemmer:\n",
    "            tokens = list(map(self.stemmer.stem, tokens))\n",
    "        \n",
    "        tokens = self.linkPreprocesser.preprocess(tokens)\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de la pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When I first saw the trailer for The Comebacks, it looked absolutely horrible and I had no interest in seeing it, but when it came out on DVD today, I figured since there was nothing else that caught my interest, I would rent it and give it a shot. I watched it tonite and it really wasn\\'t that bad. I think it was immature and stupid at times, but there were a few funny moments that made me laugh. I don\\'t really watch many sports movies, so I wonder maybe if I saw more, maybe this movie would make more sense to me, but it\\'s all good, I still didn\\'t mind so much watching The Comebacks. I admit, these \"stupid spoof\" movies are lame, but what\\'s the harm in a stupid joke every once in a while? The Comebacks isn\\'t really that bad if you give it a fair chance.Coach Fields is failing in life, family and career both, but when he is offered a chance to bring his career back to life if he can bring a looser football team into the championship. But the team is really really terrible, like beyond terrible. But with a little work and team effort they try to give it their all, even though that might turn into something more sad.The Comebacks over all isn\\'t the worst film I\\'ve seen, I think it\\'s good for a couple laughs and giggles. I know that this was stupid, but I couldn\\'t help but laugh when the coach comes in the middle of a fight in the locker room and he\\'s beating the nerd\\'s head against the locker, just him and the nerd in general were so funny together. If you have an open mind and don\\'t take this movie too seriously, I think you\\'ll have a fun time watching it, if you watch it expecting it to be Oscar worthy material, this is not the movie for you.4/10']\n",
      "[['when', 'i', 'first', 'saw', 'the', 'trailer', 'for', 'the', 'comeback', 'it', 'look', 'absolut', 'horribl', 'and', 'i', 'had', 'no', 'interest', 'in', 'see', 'it', 'but', 'when', 'it', 'came', 'out', 'on', 'dvd', 'today', 'i', 'figur', 'sinc', 'there', 'was', 'noth', 'els', 'that', 'caught', 'my', 'interest', 'i', 'would', 'rent', 'it', 'and', 'give', 'it', 'a', 'shot', 'i', 'watch', 'it', 'tonit', 'and', 'it', 'realli', 'wasnt', 'that', 'bad', 'i', 'think', 'it', 'was', 'immatur', 'and', 'stupid', 'at', 'time', 'but', 'there', 'were', 'a', 'few', 'funni', 'moment', 'that', 'made', 'me', 'laugh', 'i', 'dont', 'realli', 'watch', 'mani', 'sport', 'movi', 'so', 'i', 'wonder', 'mayb', 'if', 'i', 'saw', 'more', 'mayb', 'this', 'movi', 'would', 'make', 'more', 'sens', 'to', 'me', 'but', 'its', 'all', 'good', 'i', 'still', 'didnt', 'mind', 'so', 'much', 'watch', 'the', 'comeback', 'i', 'admit', 'these', 'stupid', 'spoof', 'movi', 'are', 'lame', 'but', 'what', 'the', 'harm', 'in', 'a', 'stupid', 'joke', 'everi', 'once', 'in', 'a', 'while', 'the', 'comeback', 'isnt', 'realli', 'that', 'bad', 'if', 'you', 'give', 'it', 'a', 'fair', 'chancecoach', 'field', 'is', 'fail', 'in', 'life', 'famili', 'and', 'career', 'both', 'but', 'when', 'he', 'is', 'offer', 'a', 'chanc', 'to', 'bring', 'his', 'career', 'back', 'to', 'life', 'if', 'he', 'can', 'bring', 'a', 'looser', 'footbal', 'team', 'into', 'the', 'championship', 'but', 'the', 'team', 'is', 'realli', 'realli', 'terribl', 'like', 'beyond', 'terribl', 'but', 'with', 'a', 'littl', 'work', 'and', 'team', 'effort', 'they', 'tri', 'to', 'give', 'it', 'their', 'all', 'even', 'though', 'that', 'might', 'turn', 'into', 'someth', 'more', 'sadth', 'comeback', 'over', 'all', 'isnt', 'the', 'worst', 'film', 'ive', 'seen', 'i', 'think', 'its', 'good', 'for', 'a', 'coupl', 'laugh', 'and', 'giggl', 'i', 'know', 'that', 'this', 'was', 'stupid', 'but', 'i', 'couldnt', 'help', 'but', 'laugh', 'when', 'the', 'coach', 'come', 'in', 'the', 'middl', 'of', 'a', 'fight', 'in', 'the', 'locker', 'room', 'and', 'hes', 'beat', 'the', 'nerd', 'head', 'against', 'the', 'locker', 'just', 'him', 'and', 'the', 'nerd', 'in', 'general', 'were', 'so', 'funni', 'togeth', 'if', 'you', 'have', 'an', 'open', 'mind', 'and', 'dont', 'take', 'this', 'movi', 'too', 'serious', 'i', 'think', 'youll', 'have', 'a', 'fun', 'time', 'watch', 'it', 'if', 'you', 'watch', 'it', 'expect', 'it', 'to', 'be', 'oscar', 'worthi', 'materi', 'this', 'is', 'not', 'the', 'movi', 'for', 'you', '410']]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline(stemming=True)\n",
    "print(train_X[:1])\n",
    "print(list(map(pipeline.preprocess, train_X[:1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des caractéristiques (features)\n",
    "\n",
    "Les méthodes suivantes permettent de transformer les textes en vecteurs utilisables pour la classification.\n",
    "\n",
    "### N-grams\n",
    "\n",
    "Une des approches répandues dans le traitement de la langue naturelle est le modèle N-Gram. Pour faire court, N-Gram est un modèle probabiliste qui a pour but de prédire le prochain élément dans une chaîne de N éléments (par exemple, de n mots précédents). Un élément peut être une lettre, une syllabe ou encore un mot. En fait, la méthode N-Gram est une forme de modèle de Markov: afin d’effectuer une prédiction, il faut se baser uniquement sur les éléments précédents de la chaîne. Il y a plusieurs variantes de N-Gram: uni-gram, bi-gram, tri-gram, etc.\n",
    "\n",
    "Voici la liste de tous les unigrams, bigrams et trigrams possible pour la phrase *\"Il nous a dit au revoir en franchissant la porte.\"* :\n",
    "- Unigram: ['Il', 'nous', 'a', 'dit', 'au', 'revoir', 'en', 'franchissant', 'la', 'porte']\n",
    "- Bigram: ['Il nous', 'nous a', 'a dit', 'dit au', 'au revoir', 'revoir en', 'en franchissant', 'franchissant la', 'la porte']\n",
    "- Trigram: ['Il nous a', 'nous a dit', 'a dit au', 'dit au revoir', 'au revoir en', 'revoir en franchissant', 'en franchissant la', 'franchissant la porte']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "\n",
    "Bag-of-words est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construit en comptant le nombre d'occurence de chaque mot. Ainsi, chaque token est ici associé à une dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Nous avons utilisé TF-IDF pour être capable de pondérer chaque mot dans un review, car nous savons tous qu'il y a des mots qui sont plus importants que d'autre pour déterminer si une review est négative ou positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Nous avons testé exhaustivement plusieurs types de classificateurs: un par régression logistique, un par descente du gradient (SGD), un par *Support Vector Machine* (SVM), un par Bayes Naïf et un par foret aléatoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def get_pipeline(classifier, tokenizer=None, preprocessor=None, ngram_range=(1, 2), use_idf=True):\n",
    "    return Pipeline([\n",
    "        ('count_vectorizer',   CountVectorizer(preprocessor=preprocessor, tokenizer=tokenizer, ngram_range=ngram_range, max_df=0.5)),\n",
    "        ('tfidf_transformer',  TfidfTransformer(smooth_idf=True, use_idf=use_idf)),\n",
    "        ('classifier',          classifier)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_test,y_predicted,label_names):\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "    figsize = (10,7)\n",
    "    df_cm = pd.DataFrame(\n",
    "        cm, index=label_names, columns=label_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_classifier(pipeline, x_test, y_test):\n",
    "    y_predicted = pipeline.predict(x_test)\n",
    "    report  = get_classifier_scores(y_test, y_predicted)\n",
    "    plot_confusion_matrix(y_test, y_predicted, [\"negatif\", \"positif\"])\n",
    "    return report\n",
    "\n",
    "def get_classifier_scores(real, pred):\n",
    "    return {\n",
    "        \"accuracy\":accuracy_score(real, pred),\n",
    "        \"f1_score\":f1_score(real, pred),\n",
    "        \"precision\":precision_score(real, pred),\n",
    "        \"recall\":recall_score(real, pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def evaluate_all_pipelines(x_train, y_train, x_valid, y_valid, classifiers, tokenizers, ngrams, use_idfs):\n",
    "\n",
    "    results = {}\n",
    "    total_iterations = len(classifiers)*len(tokenizers)*len(ngrams)*len(use_idfs)\n",
    "    with tqdm(total = total_iterations) as progress:\n",
    "        for classifier_name, classifier in classifiers:\n",
    "            results[classifier_name] = []\n",
    "            for tokenizer_name, tokenizer in tokenizers:\n",
    "                for ngram in ngrams:\n",
    "                    for use_idf in use_idfs:\n",
    "                        idf_string = \"tfidf\" if use_idf else \"bag of words\"\n",
    "                        metaparams =(tokenizer_name, ngram, idf_string) \n",
    "                        progress.write(f\"Processing : {metaparams} for {classifier_name}\")\n",
    "                        \n",
    "                        pipeline = get_pipeline(classifier, tokenizer=tokenizer, ngram_range = ngram, use_idf = use_idf)\n",
    "                        pipeline.fit(x_train, y_train)\n",
    "                        y_predict = pipeline.predict(x_valid)\n",
    "                        result = get_classifier_scores(y_valid, y_predict)\n",
    "                        results[classifier_name].append({\n",
    "                         \"result\": result,\n",
    "                         \"Metaparams\":metaparams,\n",
    "                         \"pipeline\":pipeline\n",
    "                        })\n",
    "                        \n",
    "                        progress.update(1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement des classificateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a615523f54c4475a99548bf8aaac37d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : ('Default', (1, 1), 'bag of words') for Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ImMay\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : ('Default', (1, 1), 'tfidf') for Logistic\n",
      "Processing : ('Default', (1, 2), 'bag of words') for Logistic\n",
      "Processing : ('Default', (1, 2), 'tfidf') for Logistic\n",
      "Processing : ('Our Tokenizer', (1, 1), 'bag of words') for Logistic\n",
      "Processing : ('Our Tokenizer', (1, 1), 'tfidf') for Logistic\n",
      "Processing : ('Our Tokenizer', (1, 2), 'bag of words') for Logistic\n",
      "Processing : ('Our Tokenizer', (1, 2), 'tfidf') for Logistic\n",
      "Processing : ('Default', (1, 1), 'bag of words') for SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ImMay\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : ('Default', (1, 1), 'tfidf') for SGD\n",
      "Processing : ('Default', (1, 2), 'bag of words') for SGD\n",
      "Processing : ('Default', (1, 2), 'tfidf') for SGD\n",
      "Processing : ('Our Tokenizer', (1, 1), 'bag of words') for SGD\n",
      "Processing : ('Our Tokenizer', (1, 1), 'tfidf') for SGD\n",
      "Processing : ('Our Tokenizer', (1, 2), 'bag of words') for SGD\n",
      "Processing : ('Our Tokenizer', (1, 2), 'tfidf') for SGD\n",
      "Processing : ('Default', (1, 1), 'bag of words') for SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ImMay\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : ('Default', (1, 1), 'tfidf') for SVM\n",
      "Processing : ('Default', (1, 2), 'bag of words') for SVM\n",
      "Processing : ('Default', (1, 2), 'tfidf') for SVM\n",
      "Processing : ('Our Tokenizer', (1, 1), 'bag of words') for SVM\n",
      "Processing : ('Our Tokenizer', (1, 1), 'tfidf') for SVM\n",
      "Processing : ('Our Tokenizer', (1, 2), 'bag of words') for SVM\n",
      "Processing : ('Our Tokenizer', (1, 2), 'tfidf') for SVM\n",
      "Processing : ('Default', (1, 1), 'bag of words') for Naive Bayes\n",
      "Processing : ('Default', (1, 1), 'tfidf') for Naive Bayes\n",
      "Processing : ('Default', (1, 2), 'bag of words') for Naive Bayes\n",
      "Processing : ('Default', (1, 2), 'tfidf') for Naive Bayes\n",
      "Processing : ('Our Tokenizer', (1, 1), 'bag of words') for Naive Bayes\n",
      "Processing : ('Our Tokenizer', (1, 1), 'tfidf') for Naive Bayes\n",
      "Processing : ('Our Tokenizer', (1, 2), 'bag of words') for Naive Bayes\n",
      "Processing : ('Our Tokenizer', (1, 2), 'tfidf') for Naive Bayes\n",
      "Processing : ('Default', (1, 1), 'bag of words') for Random Forest\n",
      "Processing : ('Default', (1, 1), 'tfidf') for Random Forest\n",
      "Processing : ('Default', (1, 2), 'bag of words') for Random Forest\n",
      "Processing : ('Default', (1, 2), 'tfidf') for Random Forest\n",
      "Processing : ('Our Tokenizer', (1, 1), 'bag of words') for Random Forest\n",
      "Processing : ('Our Tokenizer', (1, 1), 'tfidf') for Random Forest\n",
      "Processing : ('Our Tokenizer', (1, 2), 'bag of words') for Random Forest\n",
      "Processing : ('Our Tokenizer', (1, 2), 'tfidf') for Random Forest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "\n",
    "classifiers = [(\"Logistic\", LogisticRegression()), \n",
    "               (\"SGD\", SGDClassifier(random_state=42, max_iter=5)),\n",
    "               (\"SVM\", NuSVC()),\n",
    "               (\"Naive Bayes\",BernoulliNB()),\n",
    "               (\"Random Forest\", RandomForestClassifier(n_estimators=500))]\n",
    "tokenizer_stem = PreprocessingPipeline(stemming=True)\n",
    "tokenizers = [(\"Default\", None), (\"Our Tokenizer\", tokenizer_stem.preprocess),]\n",
    "ngrams = [(1,1), (1,2)]\n",
    "use_idfs =[False, True]\n",
    "\n",
    "results = evaluate_all_pipelines(train_X, train_Y, valid_X, valid_Y, classifiers, tokenizers, ngrams, use_idfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>tf-idf / BoW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.904314</td>\n",
       "      <td>0.905206</td>\n",
       "      <td>0.892948</td>\n",
       "      <td>0.917805</td>\n",
       "      <td>Our Tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.900392</td>\n",
       "      <td>0.900807</td>\n",
       "      <td>0.893134</td>\n",
       "      <td>0.908613</td>\n",
       "      <td>Our Tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.880261</td>\n",
       "      <td>0.878675</td>\n",
       "      <td>0.886424</td>\n",
       "      <td>0.871061</td>\n",
       "      <td>Our Tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.878954</td>\n",
       "      <td>0.874797</td>\n",
       "      <td>0.901616</td>\n",
       "      <td>0.849527</td>\n",
       "      <td>Default</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>bag of words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.813203</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.884578</td>\n",
       "      <td>0.718487</td>\n",
       "      <td>Our Tokenizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  f1_score  precision    recall      tokenizer  ngrams  \\\n",
       "SGD            0.904314  0.905206   0.892948  0.917805  Our Tokenizer  (1, 2)   \n",
       "Logistic       0.900392  0.900807   0.893134  0.908613  Our Tokenizer  (1, 2)   \n",
       "Random Forest  0.880261  0.878675   0.886424  0.871061  Our Tokenizer  (1, 2)   \n",
       "Naive Bayes    0.878954  0.874797   0.901616  0.849527        Default  (1, 2)   \n",
       "SVM            0.813203  0.792929   0.884578  0.718487  Our Tokenizer  (1, 2)   \n",
       "\n",
       "               tf-idf / BoW  \n",
       "SGD                   tfidf  \n",
       "Logistic              tfidf  \n",
       "Random Forest         tfidf  \n",
       "Naive Bayes    bag of words  \n",
       "SVM                   tfidf  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "best_results = {}\n",
    "for classifier_name, result_list in results.items():\n",
    "    result_full = max(result_list, key=lambda res: res[\"result\"]['accuracy'])\n",
    "    result = copy.deepcopy(result_full['result'])\n",
    "    result.update({\n",
    "        'tokenizer':result_full['Metaparams'][0],\n",
    "        'ngrams':result_full['Metaparams'][1],\n",
    "        'tf-idf / BoW':result_full['Metaparams'][2]\n",
    "    })\n",
    "    best_results[classifier_name] = result\n",
    "\n",
    "pd.DataFrame.from_dict(best_results, orient='index').sort_values(by=['accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultats détaillés par classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_results_please_dont_delete = copy.deepcopy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_classifier_results(selected_classifier, results):\n",
    "    result_to_display = {}\n",
    "    for result in results[selected_classifier]:\n",
    "        result_to_display[str(result['Metaparams'])] = result['result']\n",
    "\n",
    "    return pd.DataFrame.from_dict(result_to_display, orient='index').sort_values(by=['accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'tfidf')</th>\n",
       "      <td>0.900392</td>\n",
       "      <td>0.900807</td>\n",
       "      <td>0.893134</td>\n",
       "      <td>0.908613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'tfidf')</th>\n",
       "      <td>0.900131</td>\n",
       "      <td>0.900209</td>\n",
       "      <td>0.895530</td>\n",
       "      <td>0.904937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'tfidf')</th>\n",
       "      <td>0.897386</td>\n",
       "      <td>0.897693</td>\n",
       "      <td>0.891074</td>\n",
       "      <td>0.904412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'tfidf')</th>\n",
       "      <td>0.893725</td>\n",
       "      <td>0.894457</td>\n",
       "      <td>0.884467</td>\n",
       "      <td>0.904674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'bag of words')</th>\n",
       "      <td>0.892680</td>\n",
       "      <td>0.892694</td>\n",
       "      <td>0.888629</td>\n",
       "      <td>0.896796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'bag of words')</th>\n",
       "      <td>0.890327</td>\n",
       "      <td>0.890627</td>\n",
       "      <td>0.884287</td>\n",
       "      <td>0.897059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'bag of words')</th>\n",
       "      <td>0.887582</td>\n",
       "      <td>0.887699</td>\n",
       "      <td>0.882857</td>\n",
       "      <td>0.892595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'bag of words')</th>\n",
       "      <td>0.881046</td>\n",
       "      <td>0.881664</td>\n",
       "      <td>0.873261</td>\n",
       "      <td>0.890231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           accuracy  f1_score  precision  \\\n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.900392  0.900807   0.893134   \n",
       "('Default', (1, 2), 'tfidf')               0.900131  0.900209   0.895530   \n",
       "('Default', (1, 1), 'tfidf')               0.897386  0.897693   0.891074   \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.893725  0.894457   0.884467   \n",
       "('Default', (1, 2), 'bag of words')        0.892680  0.892694   0.888629   \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.890327  0.890627   0.884287   \n",
       "('Default', (1, 1), 'bag of words')        0.887582  0.887699   0.882857   \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.881046  0.881664   0.873261   \n",
       "\n",
       "                                             recall  \n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.908613  \n",
       "('Default', (1, 2), 'tfidf')               0.904937  \n",
       "('Default', (1, 1), 'tfidf')               0.904412  \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.904674  \n",
       "('Default', (1, 2), 'bag of words')        0.896796  \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.897059  \n",
       "('Default', (1, 1), 'bag of words')        0.892595  \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.890231  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_classifier = \"Logistic\"\n",
    "display_classifier_results(current_classifier, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descente Stochastique du Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'tfidf')</th>\n",
       "      <td>0.904314</td>\n",
       "      <td>0.905206</td>\n",
       "      <td>0.892948</td>\n",
       "      <td>0.917805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'tfidf')</th>\n",
       "      <td>0.903529</td>\n",
       "      <td>0.903630</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>0.908613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'tfidf')</th>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.898686</td>\n",
       "      <td>0.890234</td>\n",
       "      <td>0.907300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'bag of words')</th>\n",
       "      <td>0.894902</td>\n",
       "      <td>0.894544</td>\n",
       "      <td>0.893606</td>\n",
       "      <td>0.895483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'tfidf')</th>\n",
       "      <td>0.894248</td>\n",
       "      <td>0.895356</td>\n",
       "      <td>0.882233</td>\n",
       "      <td>0.908876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'bag of words')</th>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.893667</td>\n",
       "      <td>0.886963</td>\n",
       "      <td>0.900473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'bag of words')</th>\n",
       "      <td>0.883660</td>\n",
       "      <td>0.883994</td>\n",
       "      <td>0.877588</td>\n",
       "      <td>0.890494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'bag of words')</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.883299</td>\n",
       "      <td>0.872439</td>\n",
       "      <td>0.894433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           accuracy  f1_score  precision  \\\n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.904314  0.905206   0.892948   \n",
       "('Default', (1, 2), 'tfidf')               0.903529  0.903630   0.898701   \n",
       "('Default', (1, 1), 'tfidf')               0.898170  0.898686   0.890234   \n",
       "('Default', (1, 2), 'bag of words')        0.894902  0.894544   0.893606   \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.894248  0.895356   0.882233   \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.893333  0.893667   0.886963   \n",
       "('Default', (1, 1), 'bag of words')        0.883660  0.883994   0.877588   \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.882353  0.883299   0.872439   \n",
       "\n",
       "                                             recall  \n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.917805  \n",
       "('Default', (1, 2), 'tfidf')               0.908613  \n",
       "('Default', (1, 1), 'tfidf')               0.907300  \n",
       "('Default', (1, 2), 'bag of words')        0.895483  \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.908876  \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.900473  \n",
       "('Default', (1, 1), 'bag of words')        0.890494  \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.894433  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_classifier = \"SGD\"\n",
    "display_classifier_results(current_classifier, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'tfidf')</th>\n",
       "      <td>0.813203</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.884578</td>\n",
       "      <td>0.718487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'bag of words')</th>\n",
       "      <td>0.811373</td>\n",
       "      <td>0.788695</td>\n",
       "      <td>0.891427</td>\n",
       "      <td>0.707195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'bag of words')</th>\n",
       "      <td>0.788627</td>\n",
       "      <td>0.816020</td>\n",
       "      <td>0.719936</td>\n",
       "      <td>0.941702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'tfidf')</th>\n",
       "      <td>0.775556</td>\n",
       "      <td>0.729564</td>\n",
       "      <td>0.911452</td>\n",
       "      <td>0.608193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'tfidf')</th>\n",
       "      <td>0.743660</td>\n",
       "      <td>0.790379</td>\n",
       "      <td>0.666486</td>\n",
       "      <td>0.970851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'tfidf')</th>\n",
       "      <td>0.719477</td>\n",
       "      <td>0.776365</td>\n",
       "      <td>0.643573</td>\n",
       "      <td>0.978204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'bag of words')</th>\n",
       "      <td>0.714902</td>\n",
       "      <td>0.763372</td>\n",
       "      <td>0.650397</td>\n",
       "      <td>0.923845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'bag of words')</th>\n",
       "      <td>0.672941</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.610267</td>\n",
       "      <td>0.949055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           accuracy  f1_score  precision  \\\n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.813203  0.792929   0.884578   \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.811373  0.788695   0.891427   \n",
       "('Default', (1, 1), 'bag of words')        0.788627  0.816020   0.719936   \n",
       "('Default', (1, 2), 'tfidf')               0.775556  0.729564   0.911452   \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.743660  0.790379   0.666486   \n",
       "('Default', (1, 1), 'tfidf')               0.719477  0.776365   0.643573   \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.714902  0.763372   0.650397   \n",
       "('Default', (1, 2), 'bag of words')        0.672941  0.742857   0.610267   \n",
       "\n",
       "                                             recall  \n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.718487  \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.707195  \n",
       "('Default', (1, 1), 'bag of words')        0.941702  \n",
       "('Default', (1, 2), 'tfidf')               0.608193  \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.970851  \n",
       "('Default', (1, 1), 'tfidf')               0.978204  \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.923845  \n",
       "('Default', (1, 2), 'bag of words')        0.949055  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_classifier = \"SVM\"\n",
    "display_classifier_results(current_classifier, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesien Naif (Binomiale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'bag of words')</th>\n",
       "      <td>0.878954</td>\n",
       "      <td>0.874797</td>\n",
       "      <td>0.901616</td>\n",
       "      <td>0.849527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'tfidf')</th>\n",
       "      <td>0.878954</td>\n",
       "      <td>0.874797</td>\n",
       "      <td>0.901616</td>\n",
       "      <td>0.849527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'bag of words')</th>\n",
       "      <td>0.877516</td>\n",
       "      <td>0.874008</td>\n",
       "      <td>0.895564</td>\n",
       "      <td>0.853466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'tfidf')</th>\n",
       "      <td>0.877516</td>\n",
       "      <td>0.874008</td>\n",
       "      <td>0.895564</td>\n",
       "      <td>0.853466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'bag of words')</th>\n",
       "      <td>0.852549</td>\n",
       "      <td>0.847732</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.824580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'tfidf')</th>\n",
       "      <td>0.852549</td>\n",
       "      <td>0.847732</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.824580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'bag of words')</th>\n",
       "      <td>0.850327</td>\n",
       "      <td>0.846247</td>\n",
       "      <td>0.865897</td>\n",
       "      <td>0.827468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'tfidf')</th>\n",
       "      <td>0.850327</td>\n",
       "      <td>0.846247</td>\n",
       "      <td>0.865897</td>\n",
       "      <td>0.827468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           accuracy  f1_score  precision  \\\n",
       "('Default', (1, 2), 'bag of words')        0.878954  0.874797   0.901616   \n",
       "('Default', (1, 2), 'tfidf')               0.878954  0.874797   0.901616   \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.877516  0.874008   0.895564   \n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.877516  0.874008   0.895564   \n",
       "('Default', (1, 1), 'bag of words')        0.852549  0.847732   0.872222   \n",
       "('Default', (1, 1), 'tfidf')               0.852549  0.847732   0.872222   \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.850327  0.846247   0.865897   \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.850327  0.846247   0.865897   \n",
       "\n",
       "                                             recall  \n",
       "('Default', (1, 2), 'bag of words')        0.849527  \n",
       "('Default', (1, 2), 'tfidf')               0.849527  \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.853466  \n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.853466  \n",
       "('Default', (1, 1), 'bag of words')        0.824580  \n",
       "('Default', (1, 1), 'tfidf')               0.824580  \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.827468  \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.827468  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_classifier = \"Naive Bayes\"\n",
    "display_classifier_results(current_classifier, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forêt aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'tfidf')</th>\n",
       "      <td>0.880261</td>\n",
       "      <td>0.878675</td>\n",
       "      <td>0.886424</td>\n",
       "      <td>0.871061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'tfidf')</th>\n",
       "      <td>0.877124</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.888618</td>\n",
       "      <td>0.861082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 2), 'bag of words')</th>\n",
       "      <td>0.876078</td>\n",
       "      <td>0.874967</td>\n",
       "      <td>0.878908</td>\n",
       "      <td>0.871061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 2), 'bag of words')</th>\n",
       "      <td>0.874771</td>\n",
       "      <td>0.874509</td>\n",
       "      <td>0.872452</td>\n",
       "      <td>0.876576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'bag of words')</th>\n",
       "      <td>0.866275</td>\n",
       "      <td>0.866187</td>\n",
       "      <td>0.862914</td>\n",
       "      <td>0.869485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'bag of words')</th>\n",
       "      <td>0.865882</td>\n",
       "      <td>0.865460</td>\n",
       "      <td>0.864327</td>\n",
       "      <td>0.866597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Default', (1, 1), 'tfidf')</th>\n",
       "      <td>0.865098</td>\n",
       "      <td>0.863347</td>\n",
       "      <td>0.870726</td>\n",
       "      <td>0.856092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>('Our Tokenizer', (1, 1), 'tfidf')</th>\n",
       "      <td>0.859085</td>\n",
       "      <td>0.858382</td>\n",
       "      <td>0.858833</td>\n",
       "      <td>0.857931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           accuracy  f1_score  precision  \\\n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.880261  0.878675   0.886424   \n",
       "('Default', (1, 2), 'tfidf')               0.877124  0.874633   0.888618   \n",
       "('Default', (1, 2), 'bag of words')        0.876078  0.874967   0.878908   \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.874771  0.874509   0.872452   \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.866275  0.866187   0.862914   \n",
       "('Default', (1, 1), 'bag of words')        0.865882  0.865460   0.864327   \n",
       "('Default', (1, 1), 'tfidf')               0.865098  0.863347   0.870726   \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.859085  0.858382   0.858833   \n",
       "\n",
       "                                             recall  \n",
       "('Our Tokenizer', (1, 2), 'tfidf')         0.871061  \n",
       "('Default', (1, 2), 'tfidf')               0.861082  \n",
       "('Default', (1, 2), 'bag of words')        0.871061  \n",
       "('Our Tokenizer', (1, 2), 'bag of words')  0.876576  \n",
       "('Our Tokenizer', (1, 1), 'bag of words')  0.869485  \n",
       "('Default', (1, 1), 'bag of words')        0.866597  \n",
       "('Default', (1, 1), 'tfidf')               0.856092  \n",
       "('Our Tokenizer', (1, 1), 'tfidf')         0.857931  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_classifier = \"Random Forest\"\n",
    "display_classifier_results(current_classifier, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle retenu (meilleure exactitude): SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAHDCAYAAADiE4RPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xm8XdP5+PHPk4RGJKiiNYTEUIrGTMyKImqoooZvNUqlpn61RVulX6V8q63y7aDaGIoO5hprCmrqTxBEiDFCSWlpxRgSyX1+f5ydODLce5Jz9zn33vN5e+2Xc9ZeZ+918nLluc+z9lqRmUiSJGnB9Wr2ACRJkro7AypJkqQ6GVBJkiTVyYBKkiSpTgZUkiRJdTKgkiRJqpMBlSRJUp0MqCRJkupkQCVJklSnPs0eQLNNm3i/S8VLDbb0kP2aPQSpZb3x9rPRyPu9/++Jnfb37EJLrdzQsc8PM1SSJEl1avkMlSRJKlHbjGaPoCHMUEmSJNXJDJUkSSpPtjV7BA1hQCVJksrT1hoBlSU/SZKkOpmhkiRJpUlLfpIkSXWy5CdJkqRamKGSJEnlseQnSZJUJxf2lCRJUi3MUEmSpPJY8pMkSaqTT/lJkiSpFmaoJElSaVzYU5IkqV6W/CRJklQLM1SSJKk8lvwkSZLq5MKekiRJqoUZKkmSVB5LfpIkSXVqkaf8DKgkSVJ5WiRD5RwqSZKkOpmhkiRJ5bHkJ0mSVJ9Ml02QJElSDcxQSZKk8rTIpHQDKkmSVJ4WmUNlyU+SJKlOZqgkSVJ5LPlJkiTVyc2RJUmSVAszVJIkqTyW/CRJkurkU36SJEmqhRkqSZJUHkt+kiRJdbLkJ0mSpFqYoZIkSeVpkQyVAZUkSSpNpgt7SpIkqQZmqCRJUnks+UmSJNWpRZZNsOQnSZJUJzNUkiSpPJb8JEmS6mTJT5IkSbUwQyVJksrTIiU/M1SSJKk82dZ5Rwciom9E3B8Rj0TE+Ig4qWgfHBH3RcQzEXFpRCxctH+keD+hOD+o6lrHFe1PRcSOHd3bgEqSJPUUU4FtM3MdYF1gp4gYCvwYODMzVwMmAwcX/Q8GJmfmqsCZRT8iYk1gX2AtYCfg1xHRu70bG1BJkqTytLV13tGBrHi7eLtQcSSwLXBF0X4h8Pni9e7Fe4rz20VEFO2XZObUzHwOmABs3N69DagkSVJ5GhhQAURE74gYC7wCjAKeBV7PzOlFl0nA8sXr5YEXAYrzbwAfq26fy2fmyoBKkiR1CxExIiLGVB0jZu+TmTMyc11gBSpZpU/N5VI585LzODev9nnyKT9JklSeTlyHKjNHAiNr7Pt6RNwBDAWWiIg+RRZqBeClotskYCAwKSL6AIsDr1W1z1T9mbkyQyVJksrTwJJfRCwdEUsUrxcBtgeeAP4K7FV0Gw5cU7y+tnhPcf72zMyifd/iKcDBwGrA/e3d2wyVJEnqKZYFLiyeyOsFXJaZ10fE48AlEXEK8DBwXtH/POD3ETGBSmZqX4DMHB8RlwGPA9OBIzJzRns3NqCSJEnlaeDWM5k5DlhvLu0TmctTepn5HrD3PK51KnBqrfc2oJIkSeVxpXRJkiTVwgyVJEkqTwNLfs1kQCVJkspjyU+SJEm1MEMlSZLK0yIZKgMqSZJUnmx3x5Yew5KfJElSncxQSZKk8ljykyRJqlOLBFSW/CRJkupkhkqSJJXHhT0lSZLqZMlPkiRJtTBDJUmSytMi61AZUEmSpPK0SMnPgEqSJJWnRQIq51BJkiTVyQyVJEkqj8smSJIk1SfbWmNSuiU/SZKkOpmhkiRJ5WmRSekGVJIkqTwtMofKkp8kSVKdzFBJkqTytMikdAMqSZJUnhaZQ2XJT5IkqU5mqCRJUnlaJENlQCVJksqTrTGHypKfJElSncxQSZKk8ljykxpr6rRpHHjsqUx7/31mzGjjs1tsxBEH7Mn/nHkO4595jkwYtPwnOOXoEfRbpC+X/eU2Lr7+Vnr36kW/vn058b8PYpWVluf62//GBVfeMOu6Tz/3Ipf98oesscpKTfx2UvfSq1cv7rz7al566V/ss/chHPK1Azj88K+w8iorMXilDXntP5MB+O+jDmHvfXYDoE+fPqy++iqsMmgjJk9+o5nDV1fSIssmRHbT2mZELAHsn5m/Lt4vB/wiM/cq3l8MrAX8LjPPnNd1pk28v3v+AfRAmcm7702l3yJ9eX/6dIYf80O+87UDWGXF5em/6CIA/GTkH1lyicX46hd35e133p3V/tfRD3Hp9bfym1O+/aFrPv3ci/z3yWdy0+/OaPj30bwtPWS/Zg9BHTjiyINYb/1PM2BAf/bZ+xCGDFmT119/g+tv/BPbbPX5WQFVtZ2GbcsRRx7Erp/7UhNGrFq98faz0cj7TTn9q53292y/Y85t6NjnR3eeQ7UEcPjMN5n5UlUw9Qlgs8wc0l4wpa4lIui3SF8Apk+fwfTpM4hgVtCUmUydOo2g8vM0sx3g3femQsz5c3bjnfey89abNmD0Us+x3HKfYMedPsNFF142q23cuMd54YV/tPu5vfbelSsuv67s4am7ybbOO7qw0gKqiBgUEU9ExDkRMT4ibomIRSJilYi4KSIejIi7I2KNov8qETE6Ih6IiJMj4u2ivX9E3BYRD0XEoxGxe3GL04BVImJsRPy0uN9jxblbgGWKc1uW9R3V+WbMaGOvI45n6/2OYOh6azNkjVUBOOGMkWyz/5E8N+ll9t/ts7P6X3zdKIZ95WjOOO8Sjjv0gDmud9Od9zFsm6ENG7/UE5z2kxP4nxN+TNt8zH1ZZJG+bL/9Vlx7zU0ljkzdUlt23tGFlZ2hWg04KzPXAl4H9gRGAl/PzA2AY4BfF31/Dvw8MzcCXqq6xnvAHpm5PvAZ4GcREcB3gWczc93MPHa2++5Wde7u2QcVESMiYkxEjDn34qs679uqbr179+KKs07l1t//nMeensgzz78IwCnfGsHtf/glKw9cjpvuum9W//12/Sw3/u5nfPOgfRh58TUfuta4JyfQt+/CrDZoYEO/g9Sd7bjTZ3j11f8wduxjHXeuMmzn7Rg9+kHnTqlllR1QPZeZY4vXDwKDgM2AyyNiLPBbYNni/KbA5cXrP1VdI4D/jYhxwK3A8sDH6xlUZo7MzA0zc8Ov7rdHPZdSSRbrvygbDVmDv40ZN6utd+9e7LjVJtz6twfm6D9s66Hcfu+DH2q78c7Rlvuk+TR06AYM23k7xo2/k/Mv+Dlbbb0pI8/9WYef+8Jeu1ju01xlW1unHV1Z2QHV1KrXM4AlgdeLzNHM41MdXOO/gKWBDTJzXeBfQN9yhqtmeu31N3nz7XcAeG/qNEY/PJ5BKyzLCy/9C6jMobrzvocZvEIlBv/7P/4567N33T+WFZf/xKz3bW1t3HL3/ey0teU+aX6c9IPTWXP1LRiy1tYcdOBR3HXnvYz46tHtfmaxxfqzxeYbc8Nfbm3QKNWttEjJr9HLJrwJPBcRe2fm5UXpbkhmPgKMplISvBTYt+oziwOvZOb7EfEZYOaz728BAxo4dpXs1cmvc8LpI5nR1kZmGztsuQlbbbwuw489hbenvAuZfHLwinz/yK8AlflTox8eT58+vVms/6KcevSIWdd68LGn+MRSSzJw2WWa9XWkHuVrhw3nqG8cwsc/vjT/b/RfGHXzHXz9yO8BsMuuO3L77fcwZcq7TR6l1DylLZsQEYOA6zNz7eL9MUB/4ELgbCqlvoWASzLz5IhYDfgDlRLfX4ARmbl8RCwFXFf0HQtsDgzLzOcj4k/AEOBG4KyZ95v93u1x2QSp8Vw2QWqeRi+b8M4pX+q0v2cXPeEPXXbZhNIyVJn5PLB21fvTq07vNJeP/AMYmpkZEfsCY4rP/ZvK/Kq53WP/2ZrWntu9JUlSk3TxUl1n6UorpW8A/KooA74OHNTk8UiSJNWkywRUxfIG6zR7HJIkqRN18afzOkuXCagkSVIP1CIlv+689YwkSVKXYIZKkiSVp4vvwddZDKgkSVJ5LPlJkiSpFmaoJElSabr6HnydxYBKkiSVx5KfJEmSamGGSpIkladFMlQGVJIkqTwtsmyCJT9JkqQ6maGSJEnlseQnSZJUn2yRgMqSnyRJUp3MUEmSpPK0SIbKgEqSJJWnRVZKt+QnSZJUJzNUkiSpPJb8JEmS6tQiAZUlP0mSpDoZUEmSpNJkZqcdHYmIgRHx14h4IiLGR8RRs50/JiIyIpYq3kdE/CIiJkTEuIhYv6rv8Ih4pjiGd3RvS36SJKk8jS35TQeOzsyHImIA8GBEjMrMxyNiIPBZ4IWq/sOA1YpjE+BsYJOIWBI4EdgQyOI612bm5Hnd2AyVJEnqETLz5cx8qHj9FvAEsHxx+kzg21QCpJl2By7KitHAEhGxLLAjMCozXyuCqFHATu3d2wyVJEkqT5MmpUfEIGA94L6I2A34R2Y+EhHV3ZYHXqx6P6lom1f7PBlQSZKk0nTmXn4RMQIYUdU0MjNHzqVff+BK4BtUyoDHAzvM7ZJzact22ufJgEqSJHULRfA0RwBVLSIWohJM/TEz/xwRnwYGAzOzUysAD0XExlQyTwOrPr4C8FLRvs1s7Xe0d1/nUEmSpPK0ZecdHYhKxHQe8ERmngGQmY9m5jKZOSgzB1EJltbPzH8C1wJfLp72Gwq8kZkvAzcDO0TERyPio1SyWze3d28zVJIkqTyN3cpvc+AA4NGIGFu0fS8zb5hH/xuAnYEJwBTgKwCZ+VpE/BB4oOh3cma+1t6NDagkSVJpOnMOVYf3yryHuc9/qu4zqOp1AkfMo9/5wPm13tuSnyRJUp3MUEmSpPK0yF5+BlSSJKk8jZ1D1TSW/CRJkupkhkqSJJWmkZPSm8mASpIklceSnyRJkmphhkqSJJXGkp8kSVK9LPlJkiSpFmaoJElSabJFMlQGVJIkqTwtElBZ8pMkSaqTGSpJklQaS36SJEn1apGAypKfJElSncxQSZKk0ljykyRJqlOrBFSW/CRJkupkhkqSJJWmVTJUBlSSJKk8Gc0eQUNY8pMkSaqTGSpJklQaS36SJEl1yjZLfpIkSaqBGSpJklQaS36SJEl1Sp/ykyRJUi3MUEmSpNJY8pMkSaqTT/lJkiSpJmaoJElSaTKbPYLGMKCSJEmlseQnSZKkmpihkiRJpWmVDNU8A6qIWKy9D2bmm50/HEmS1JM4hwrGAwlUh5Yz3yewYonjkiRJ6jbmGVBl5sBGDkSSJPU8rVLyq2lSekTsGxHfK16vEBEblDssSZLUE2RGpx1dWYcBVUT8CvgMcEDRNAX4TZmDkiRJ6k5qecpvs8xcPyIeBsjM1yJi4ZLHJUmSegD38vvA+xHRi8pEdCLiY0CL/PFIkqR6tHXxUl1nqWUO1VnAlcDSEXEScA/w41JHJUmS1I10mKHKzIsi4kFg+6Jp78x8rNxhSZKknqCrTybvLLWulN4beJ9K2c/taiRJUk1cNqEQEccDFwPLASsAf4qI48oemCRJUndRS4bqS8AGmTkFICJOBR4EflTmwCRJUvfn1jMf+Pts/foAE8sZjiRJ6klapeTX3ubIZ1KZMzUFGB8RNxfvd6DypJ8kSZJoP0M180m+8cBfqtpHlzccSZLUk7TKOlTtbY58XiMHIkmSeh6XTShExCrAqcCaQN+Z7Zn5yRLHJUmS1G3UsqbUBcDvgACGAZcBl5Q4JkmS1ENkdt7RldUSUPXLzJsBMvPZzDwB+Ey5w5IkST1BW0anHV1ZLcsmTI2IAJ6NiEOBfwDLlDssSZLUEziH6gPfBPoD/01lLtXiwEFlDkqSJKk7qWVz5PuKl28BB5Q7HEmS1JN09blPnaW9hT2vorKQ51xl5hdKGZEkSeoxuvrcp87SXobqVw0bRRP1W2OPZg9BajnvvnR3s4cgSZ2qvYU9b2vkQCRJUs/jpHRJkqQ6tUrJr5Z1qCRJktSOmjNUEfGRzJxa5mAkSVLP0iIP+XWcoYqIjSPiUeCZ4v06EfHL0kcmSZK6vVZZKb2Wkt8vgF2A/wBk5iO49YwkSepiIuL8iHglIh6rals3IkZHxNiIGBMRGxftERG/iIgJETEuItav+szwiHimOIbXcu9aAqpemfn32dpm1HJxSZLU2jKj044aXADsNFvbT4CTMnNd4H+K9wDDgNWKYwRwNkBELAmcCGwCbAycGBEf7ejGtQRULxbRXEZE74j4BvB0DZ+TJEktrq0Tj45k5l3Aa7M3A4sVrxcHXipe7w5clBWjgSUiYllgR2BUZr6WmZOBUcwZpM2hlknph1Ep+60I/Au4tWiTJElqmIgYQSWbNNPIzBzZwce+AdwcEadTSSRtVrQvD7xY1W9S0Tav9nbVspffK8C+HfWTJEmaXdJ5k8mL4KmjAGp2hwHfzMwrI+KLwHnA9jDXgWU77e3qMKCKiHPmdqHMHDGX7pIkSbO0NX/dhOHAUcXry4Fzi9eTgIFV/VagUg6cBGwzW/sdHd2kljlUtwK3FcffgGUA16OSJEndwUvA1sXrbSmWgQKuBb5cPO03FHgjM18GbgZ2iIiPFpPRdyja2lVLye/S6vcR8XsqE7QkSZLa1daJJb+ORMTFVLJLS0XEJCpP6x0C/Dwi+gDv8cEcrBuAnYEJwBTgKwCZ+VpE/BB4oOh3cmbOPtF9Dguyl99gYKUF+JwkSWoxnTmHqsN7Ze43j1MbzKVvAkfM4zrnA+fPz71rmUM1mQ/mUPWi8jjid+fnJpIkST1ZuwFVRASwDvCPoqmtiOgkSZI6VMv6UT1Bu5PSi+DpqsycURwGU5IkqWZJdNrRldXylN/91fvbSJIk6cPmWfKLiD6ZOR3YAjgkIp4F3qGy4FVmpkGWJElqV6uU/NqbQ3U/sD7w+QaNRZIk9TAGVMXS65n5bIPGIkmS1C21F1AtHRHfmtfJzDyjhPFIkqQepKtPJu8s7QVUvYH+zH2TQEmSpA61tUgU0V5A9XJmntywkUiSJHVTHc6hkiRJWlCN3MuvmdoLqLZr2CgkSVKP1Corgs9zYc9adlaWJElSDZsjS5IkLSjXoZIkSapTW7TGHKpa9vKTJElSO8xQSZKk0rTKpHQDKkmSVJpWmUNlyU+SJKlOZqgkSVJp3HpGkiSpTq2yUrolP0mSpDqZoZIkSaXxKT9JkqQ6tcocKkt+kiRJdTJDJUmSStMq61AZUEmSpNK0yhwqS36SJEl1MkMlSZJK0yqT0g2oJElSaVplDpUlP0mSpDqZoZIkSaVplQyVAZUkSSpNtsgcKkt+kiRJdTJDJUmSSmPJT5IkqU4GVJIkSXVypXRJkiTVxAyVJEkqjSulS5Ik1alV5lBZ8pMkSaqTGSpJklSaVslQGVBJkqTS+JSfJEmSamKGSpIklcan/CRJkurUKnOoLPlJkiTVyQyVJEkqTatMSjegkiRJpWlrkZDKkp8kSVKdzFBJkqTStMqkdAMqSZJUmtYo+FnykyRJqpsZKkmSVBpLfpIkSXVqlZXSLflJkiTVyQyVJEkqTausQ2VAJUmSStMa4ZQlP0mSpLqZoZIkSaXxKT9JkqQ6tcocKkt+kiRJdTJDJUmSStMa+SkzVJIkqURtnXh0JCLOj4hXIuKxqrafRsSTETEuIq6KiCWqzh0XERMi4qmI2LGqfaeibUJEfLeW72lAJUmSeooLgJ1maxsFrJ2ZQ4CngeMAImJNYF9greIzv46I3hHRGzgLGAasCexX9G2XJT9JklSaRk5Kz8y7ImLQbG23VL0dDexVvN4duCQzpwLPRcQEYOPi3ITMnAgQEZcUfR9v795mqCRJUmmyE49OcBBwY/F6eeDFqnOTirZ5tbfLgEqSJHULETEiIsZUHSPm47PHA9OBP85smku3bKe9XZb8JElSaTpzYc/MHAmMnN/PRcRwYBdgu8ycGRxNAgZWdVsBeKl4Pa/2eTJDJUmSSpOd+M+CiIidgO8Au2XmlKpT1wL7RsRHImIwsBpwP/AAsFpEDI6IhalMXL+2o/uYoZIkST1CRFwMbAMsFRGTgBOpPNX3EWBURACMzsxDM3N8RFxGZbL5dOCIzJxRXOdI4GagN3B+Zo7v6N4GVJIkqTSN3MsvM/ebS/N57fQ/FTh1Lu03ADfMz70NqCRJUmncy0+SJEk1MUMlSZJK0xr5KQMqSZJUIkt+UhOtsMJy3HrL5Tw67g4eGXs7Xz/yYACGDFmTe+66locfupWrr7qAAQP6A7DSSivw1hsTGPPALYx54BbO+tVpzRy+1K1MnTqNfb96FF8Yfji7/9fX+NW5v//Q+f8949dstP0es95f/ZdRbPm5fdhz+BHsOfwIrrj2plnnvvatE9h0x704/NgTGzZ+qSvodhmqiDgUmJKZF0XEgcAtmflSce5c4IzMfDwi9gZOBv6ZmZ9p3oi1IKZPn86x3z6Jh8c+Rv/+i3L/fTdx62138dvf/JTvfOeH3HX3aA4cvg/HHH0YJ/7gpwA8O/HvbLjRDk0eudT9LLzwQpz/i9Po128R3p8+nS8fdgxbDt2Qddb+FI898TRvvv3OHJ/ZadutOf7ow+do/8r+e/Lee1O57Job5zin1tTIp/yaqdtlqDLzN5l5UfH2QGC5qnNfzcyZmxceDBxuMNU9/fOfr/Dw2McAePvtd3jyyWdYfrlPsPonV+Guu0cDcOttd7PHHjs3c5hSjxAR9Ou3CFD5ZWb69OlEBDNmzOBnZ53H0YcfXPO1hm64Hv369StrqOqGmr2wZ6M0NKCKiEER8WREXBgR4yLiiojoFxHbRcTDEfFoRJwfER8p+p8WEY8XfU8v2n4QEcdExF7AhsAfI2JsRCwSEXdExIYR8T/AFsBvIuKnjfyO6nwrrbQC666zNvfd/zDjxz/FrrtWslB77bkLA1eYFU8zeNCKPHD/zdx+6xVssfnG87qcpLmYMWMGew4/gq122Y9NN1qPIWutwZ+uvI7PbDGUpZdaco7+o+68hz2+fBjfPP4UXv7Xq00YsdS1NCNDtTowMjOHAG8C3wIuAPbJzE9TKUMeFhFLAnsAaxV9T6m+SGZeAYwB/isz183Md6vOnVx17tgGfCeVZNFF+3HZpefwrWNO5K233uarI77F4YceyH2jb2TAgEWZNu19AF5++RUGr7IxG228I8ccexK/v+isWfOrJHWsd+/eXHnhWdx21e959PGnGTP2UW75693sv9duc/TdZotNuOWKC7jqorMZuuF6HH/Kz5owYnUXbZ14dGXNCKhezMy/Fa//AGwHPJeZTxdtFwJbUQm23gPOjYgvAFPmuNICqt6tuq1tzrkB6hr69OnD5Zeew8UXX8XVV1fmYzz11LMM+9z+bDJ0GJdceg0TJz4PwLRp03jttckAPPTwo0yc+DyfXG3lZg1d6rYWG9CfjdYfwv0PjeOFSS+z8z4HscOew3nvvakM++JBACyx+GIsvPDCAOy12048/tQzzRyyujhLfuWp6U8kM6cDGwNXAp8Hbmr/E/MxgMyRmblhZm7Yq9einXVZdbJzRv6MJ56cwP/9/IONxZde+mNAZc7H9447it+OrDyNtNRSS9KrV+U/58GDV2TVVQcz8bkXGj9oqRt6bfLrvPnW2wC8N3Uqox94mDVXX5U7r/sTt1x5IbdceSF9+36EGy87H4BX//3arM/+9Z7RrLzSwKaMW+pKmvGU34oRsWlm3gvsB9wKfC0iVs3MCcABwJ0R0R/ol5k3RMRoYMJcrvUWMKBhI1fDbL7ZRhzwpb0Y9+jjjHngFgC+//3TWHXVwRx22IEAXH31DVxw4aUAbLnlUH5w4jFMnz6DGTNmcMSRxzF58uvNGr7Urbz6n8kcf8rpzGhrI9uSHbfdkm0232Se/f9w+TXccc9oevfpzeIDBnDKCUfPOvflw47huRdeZMqU99ju81/i5OO+yeabbNCIr6EuqquX6jpLZDYuhRYRg6hsNngXsBnwDJUAalPgdCoB3gPAYcCSwDVAXyCA0zPzwoj4AfB2Zp4eEXsC/wu8W1zjRuCYzBwTEXfMfN3emPosvHzXziFKPdC7L93d7CFILWuhpVaORt7vgJW+0Gl/z/7+739u6NjnRzMyVG2ZeehsbbcB683W9jKVkt+HZOYPql5fSaUkONM2Vee2QZIkqQG63cKekiSp+2iVMlBDA6rMfB5Yu5H3lCRJzeNefpIkSaqJJT9JklSarr5+VGcxoJIkSaVplWUTDKgkSVJpnEMlSZKkmpihkiRJpXEOlSRJUp1aZQ6VJT9JkqQ6maGSJEmlaeSewc1kQCVJkkrjU36SJEmqiRkqSZJUmlaZlG5AJUmSStMqyyZY8pMkSaqTGSpJklSaVpmUbkAlSZJK0yrLJljykyRJqpMZKkmSVBqf8pMkSaqTT/lJkiSpJmaoJElSaXzKT5IkqU4+5SdJkqSamKGSJEmlseQnSZJUJ5/ykyRJUk3MUEmSpNK0tcikdAMqSZJUmtYIpyz5SZIk1c0MlSRJKo1P+UmSJNWpVQIqS36SJEl1MkMlSZJK0ypbzxhQSZKk0ljykyRJUk3MUEmSpNK0ytYzBlSSJKk0rTKHypKfJElSncxQSZKk0rTKpHQDKkmSVBpLfpIkSaqJGSpJklQaS36SJEl1apVlEyz5SZIk1ckMlSRJKk1bi0xKN6CSJEmlseQnSZKkmhhQSZKk0rRldtpRi4hYIiKuiIgnI+KJiNg0IpaMiFER8Uzx748WfSMifhEREyJiXESsv6Df04BKkiSVJjvxnxr9HLgpM9cA1gGeAL4L3JaZqwG3Fe8BhgGrFccI4OwF/Z4GVJIkqUeIiMWArYDzADJzWma+DuwOXFh0uxD4fPF6d+CirBgNLBERyy7IvZ2ULkmSStPgp/xWBl4FfhcR6wAPAkcBH8/MlwEy8+WIWKbovzzwYtXnJxVtL8/vjc1QSZKk0nRmyS8iRkTEmKpjxGy36wOsD5ydmesB7/BBeW9uYq5DXgBmqCRJUreQmSOBke10mQRMysz7ivdXUAmo/hURyxbZqWWBV6r6D6z6/ArASwsyNjNUkiSpNI18yi8z/wm8GBGrF03bAY8D1wLDi7bhwDXF62uBLxdP+w0F3phZGpxfZqgkSVJpmrCw59eBP0bEwsBE4CtUEkiXRcTBwAtOjV97AAALhUlEQVTA3kXfG4CdgQnAlKLvAjGgkiRJPUZmjgU2nMup7ebSN4EjOuO+BlSSJKk0mW3NHkJDGFBJkqTStLXIXn4GVJIkqTTZ2HWomsan/CRJkupkhkqSJJXGkp8kSVKdLPlJkiSpJmaoJElSaRq8OXLTGFBJkqTSNGGl9Kaw5CdJklQnM1SSJKk0rTIp3YBKkiSVplWWTbDkJ0mSVCczVJIkqTSW/CRJkurUKssmWPKTJEmqkxkqSZJUGkt+kiRJdfIpP0mSJNXEDJUkSSqNJT9JkqQ6+ZSfJEmSamKGSpIklSZbZFK6AZUkSSqNJT9JkiTVxAyVJEkqjU/5SZIk1alV5lBZ8pMkSaqTGSpJklQaS36SJEl1apWAypKfJElSncxQSZKk0rRGfgqiVVJx6nkiYkRmjmz2OKRW48+eNCdLfurORjR7AFKL8mdPmo0BlSRJUp0MqCRJkupkQKXuzDkcUnP4syfNxknpkiRJdTJDJUmSVCcDKvVYEfHRiOjb7HFIrSQiotljkJrBgEo9UkSsAVwArNjkoUgtIyI+Dfw6InoV7w2u1DIMqNSjRMVCwNHAXcDzEbFmk4cl9XgRsShwOjAaGBARC6eTdNVCDKjU00Rmvg9cDHwbmAi83twhSS2hF5Wft1WBy4HlmjscqbEMqNRjRMTHgbOLDNVzwADgXWCR4rz/vUudLCIGRsSnM/Mt4Bkq2eH7MvP55o5Maiz/glFP8hpwBrA88A/gE8D3gasiYrPMbDOokjrdtkCfiFiEDwKqjSNifx8KUSvxLxd1exHRB6Ao9b0AHAbcDfTKzEuA3wK/jIgtMrOteSOVeo6ZE84z80LgeeAK4PXMPBv4JZX9/nY3qFKr6NPsAUj1KIKpfSJiHBDA7sCpwELAnyNij8w8qygD/jYiNgfecLKstOAioh+VuVLjImIr4FHgXuA7ETEjM6+PiBnAyUDviLjYnzn1dK6Urm4vIjYFrgKmAdtl5jPFb8WnAWsC+2Tm5IhYMTNfaOZYpe6u+OWkP/BTKj9zuwC7ZuYjEfEdYBvgpMwcHRG7AP/OzNFNG7DUIJb81BM8B7xI5X/uSxVtU4FjqczpuK7IZL3YnOFJPUNELAMcmJmTgVHAAcBlmfkIQGb+GLgD+ElEbJ6Z1xtMqVWYoVK3FBGRmRkRCxVzp4iIYcBPgBMy85qIWBl4D1g0M59p5nilniAihlB5cvYNYCMqv8QcDVwLXJKZrxX9DgfGZOb9zRqr1GjOoVK3UxVM7U5l/lRf4AeZeWNEDADOKP7HvyNwaGY+1tQBSz1EZo4rSn6nUckC/xB4FTgTeDcipgL7AXtm5rTmjVRqPEt+6naKYGoYlSURjqPyi8E1EbFNZl4GHAp8EvihwZRUv5lP9EXEWsDCVBbu7ENl8dwXgG8CWwNfAf5gMKVWZMlP3UpVdup7VOZwLAd8A7gdOAIYnpk3zywFzuzfzDFLPUFE7EYlgPpmZj4QEUOBfYDJwDnAv4DFiwdA/LlTyzGgUrcSEWtk5pPF62WBPwCHZebTEXEnldXRtysmzUrqBEVm6mLgC5k5ISI+BiSVXQi+TyWY+nFmTmniMKWmcg6VuryqrNRqwP0RcVFmHpmZL0fEP4BNImJpKk/0nWMwJXWOqkzTx4FXgGUiYn9gC2BjYENgJPCuwZRanXOo1OUVwdQuVH4T/jWwR0SMLE7fBWwH/B64MjPva9IwpR5j5pwp4GPFv/8KjAF+TmUD5C9S2eZpo8x8KDOfaPwopa7Fkp+6vIhYFPgL8LPMvC4iPgrcD1yemd+LiN7AKpn5dFMHKvUgEbET8C3gn1S2ljkjM18vzm0CXAgclJn/r2mDlLoQS37q8jLznYh4DnipeD85Io4CLosIMvN7gMGU1EmKOVO/ovLU3gAqpb3fRMTRVBbPvQg42mBK+oAlP3U5VY9orx4RAyOiP5WM1B+LPcSg8mTRmcB2EbFlk4Yq9RhVZT6AjwCjMvNu4CbgfOAtYA0q+/btUezXF3NeSWpNZqjU5VStM/VjKjvY7wesDawF3B0RtwF7U9kIuS/Q1qyxSj1F8XO3ObAylb8b9o6IazPzRmBSREwHVsrMNuDxmZ9p3oilrsWASl1ORKwKnAjsAWxCJWDql5lHRsS2QD/gXCpPHn0WOLtZY5W6u6qnaIdS+VkaR2Xe1CTgpIgYSCWA2oxKqU/SXDgpXV1C9UKAxRo3+1NZ2+YYYP9i7ZsdgNGZ+WYxx+M84GszN2aVtGAiYmPgR8D3MvO+Yh/MLwKbU1lr6u/AdZl5dROHKXVpZqjUJRS/IW8NfIrKY9nfpPLf5yrFiudDge8ChwBvUvnt+XOZ+Z9mjVnqQRYHtqGyBMl9VLaTGU9l2YTvFGW+D/3iI+nDDKjUVFXlhk2orDH1FPAEcDXwZeDIYu7GQVQ2QH4WIDPfaNaYpZ4mM0dFxBeAn0XEc5l5cUS8QSXIWioiXs1Cc0cqdV2W/NR0RbnhZODbxW72BwArActSedroMWB88T99f0OWShIRuwJ/BG4EplBZLPf65o5K6h5cNkFdwRLA9lQmmENlz7CJVB7TfjQz/y8zR4FPFUllyszrgC8Bq1H52bs+Ck0emtTlWfJT02XmLUW54UcR8VJRbri0OO2Ec6mBMvPaiHgPOD8ins/MPzd7TFJ3YMlPXUZE7Az8EPhFZl7Y7PFIrSwiPgs8m5kTmz0WqTswoFKXEhG7AadRKQH+c+bTRZIkdWUGVOpyImLpzHy12eOQJKlWBlSSJEl18ik/SZKkOhlQSZIk1cmASpIkqU4GVJIkSXUyoJJaQETMiIixEfFYRFweEf3quNY2EXF98Xq3iPhuO32XiIjDF+AeP4iIY2ptn63PBRGx13zca1BEPDa/Y5SkagZUUmt4NzPXzcy1gWnAodUni91F5vv/B5l5bWae1k6XJYD5DqgkqbsxoJJaz93AqkVm5omI+DXwEDAwInaIiHsj4qEik9UfICJ2iognI+Ie4AszLxQRB0bEr4rXH4+IqyLikeLYjMoirasU2bGfFv2OjYgHImJcRJxUda3jI+KpiLgVWL2jLxERhxTXeSQirpwt67Z9RNwdEU9HxC5F/94R8dOqe3+t3j9ISZrJgEpqIRHRBxgGPFo0rQ5clJnrAe8AJwDbZ+b6wBjgWxHRFzgH2BXYEvjEPC7/C+DOzFwHWB8YD3yXyvYl62bmsRGxA5WNdzcG1gU2iIitImIDYF9gPSoB20Y1fJ0/Z+ZGxf2eAA6uOjcI2Br4HPCb4jscDLyRmRsV1z8kIgbXcB9J6pCbI0utYZGIGFu8vhs4D1gO+Htmji7ahwJrAn+LCICFgXuBNYDnMvMZgIj4AzBiLvfYFvgyQGbOAN6IiI/O1meH4ni4eN+fSoA1ALgqM6cU97i2hu+0dkScQqWs2B+4uercZcW2Rc9ExMTiO+wADKmaX7V4ce+na7iXJLXLgEpqDe9m5rrVDUXQ9E51EzAqM/ebrd+6QGdtqRDAjzLzt7Pd4xsLcI8LgM9n5iMRcSCwTdW52a+Vxb2/npnVgRcRMWg+7ytJc7DkJ2mm0cDmEbEqQET0i4hPAk8CgyNilaLffvP4/G3AYcVne0fEYsBbVLJPM90MHFQ1N2v5iFgGuAvYIyIWiYgBVMqLHRkAvBwRCwH/Ndu5vSOiVzHmlYGninsfVvQnIj4ZEYvWcB9J6pAZKkkAZOarRabn4oj4SNF8QmY+HREjgL9ExL+Be4C153KJo4CREXEwMAM4LDPvjYi/FcsS3FjMo/oUcG+RIXsb+FJmPhQRlwJjgb9TKUt25PvAfUX/R/lw4PYUcCfwceDQzHwvIs6lMrfqoajc/FXg87X96UhS+9wcWZIkqU6W/CRJkupkQCVJklQnAypJkqQ6GVBJkiTVyYBKkiSpTgZUkiRJdTKgkiRJqpMBlSRJUp3+P01Ov3ajKrTBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9050666666666667,\n",
       " 'f1_score': 0.9064880483320199,\n",
       " 'precision': 0.8921923474663909,\n",
       " 'recall': 0.9212493326214629}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline = max(results['SGD'], key=lambda res: res[\"result\"]['accuracy'])['pipeline']\n",
    "evaluate_classifier(best_pipeline, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de termes (pas dans l'analyse, car jugé non-intéressant pour l'étude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def is_valid_word_pos(word_pos):\n",
    "    pos_filter = [\"NOUN\", \"ADJ\"]\n",
    "    \n",
    "    if word_pos is not None:\n",
    "        word, pos = word_pos\n",
    "        return len(word) > 2 and pos in pos_filter\n",
    "    return False\n",
    "    \n",
    "    \n",
    "def extract_frequent_words(reviews, use_single_words):\n",
    "    \n",
    "    frequency_distribution = nltk.FreqDist()\n",
    "    total_words = 0\n",
    "    for review in tqdm(reviews):\n",
    "        pp = PreprocessingPipeline(True)\n",
    "        tokens = pp.preprocess(review.lower())\n",
    "\n",
    "        pos_pairs = nltk.pos_tag(tokens, tagset='universal') \n",
    "        \n",
    "        prev_word_pos = None\n",
    "        prev_prev_word_pos = None\n",
    "        for word, pos in pos_pairs:\n",
    "            if is_valid_word_pos((word, pos)):\n",
    "                if use_single_words:\n",
    "                    frequency_distribution[word] += 1\n",
    "                \n",
    "                if pos == \"NOUN\" and is_valid_word_pos(prev_word_pos):\n",
    "                    frequency_distribution[\"{} {}\".format(prev_word_pos[0], word)] += 1\n",
    "                    if is_valid_word_pos(prev_prev_word_pos):\n",
    "                        frequency_distribution[\"{} {} {}\".format(prev_prev_word_pos[0],prev_word_pos[0], word)] += 1\n",
    "            \n",
    "            prev_prev_word_pos = prev_word_pos\n",
    "            prev_word_pos = (word, pos)\n",
    "            \n",
    "            total_words += 1\n",
    "    return [(word,\"{:.2}%\".format(100 *frequency/total_words)) for word, frequency in frequency_distribution.most_common(100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "positive_reviews = [review for idx, review in enumerate(X) if y[idx]]\n",
    "negative_reviews = [review for idx, review in enumerate(X) if not y[idx]]\n",
    "reviews ={\n",
    "    \"positive\": positive_reviews[:],\n",
    "    \"negative\": negative_reviews[:]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3f36034e4b4265aa6f9c2cd541278a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2008b9f369c49febaace3b2cb234063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "review_frequent_words = {}\n",
    "for sentiment, review in reviews.items():\n",
    "    review_frequent_words[sentiment] = extract_frequent_words(review, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termes plus fréquents (excluant les termes singuliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(first time, 0.016%)</td>\n",
       "      <td>(special effect, 0.025%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(new york, 0.015%)</td>\n",
       "      <td>(worst movi, 0.019%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(main charact, 0.014%)</td>\n",
       "      <td>(main charact, 0.018%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(special effect, 0.014%)</td>\n",
       "      <td>(bad movi, 0.016%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(same time, 0.013%)</td>\n",
       "      <td>(horror movi, 0.016%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(high recommend, 0.011%)</td>\n",
       "      <td>(low budget, 0.015%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(great movi, 0.011%)</td>\n",
       "      <td>(horror film, 0.013%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(good movi, 0.011%)</td>\n",
       "      <td>(bad guy, 0.013%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(horror film, 0.01%)</td>\n",
       "      <td>(only thing, 0.012%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(real life, 0.01%)</td>\n",
       "      <td>(good movi, 0.012%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(great film, 0.0096%)</td>\n",
       "      <td>(bad act, 0.012%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(high school, 0.0093%)</td>\n",
       "      <td>(worst film, 0.01%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(long time, 0.0081%)</td>\n",
       "      <td>(high school, 0.0098%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(horror movi, 0.0078%)</td>\n",
       "      <td>(good thing, 0.0094%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(bad guy, 0.0075%)</td>\n",
       "      <td>(whole movi, 0.0094%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(stori line, 0.0075%)</td>\n",
       "      <td>(whole thing, 0.0086%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(other film, 0.0073%)</td>\n",
       "      <td>(stori line, 0.0082%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(love stori, 0.0072%)</td>\n",
       "      <td>(new york, 0.008%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(good job, 0.0072%)</td>\n",
       "      <td>(only reason, 0.0076%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(everi time, 0.0071%)</td>\n",
       "      <td>(first time, 0.0073%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(great job, 0.007%)</td>\n",
       "      <td>(sex scene, 0.0069%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(low budget, 0.0069%)</td>\n",
       "      <td>(bad film, 0.0067%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(best film, 0.0067%)</td>\n",
       "      <td>(product valu, 0.0066%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(good film, 0.0067%)</td>\n",
       "      <td>(real life, 0.0064%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(worth watch, 0.0064%)</td>\n",
       "      <td>(same time, 0.0063%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(world war, 0.0064%)</td>\n",
       "      <td>(long time, 0.0061%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(young man, 0.0063%)</td>\n",
       "      <td>(other movi, 0.006%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(best movi, 0.0062%)</td>\n",
       "      <td>(everi time, 0.0058%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(mani time, 0.0062%)</td>\n",
       "      <td>(mani time, 0.0057%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(mani peopl, 0.0062%)</td>\n",
       "      <td>(fight scene, 0.0057%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>(action movi, 0.0035%)</td>\n",
       "      <td>(someth els, 0.0037%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>(last night, 0.0034%)</td>\n",
       "      <td>(video game, 0.0037%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>(favorit movi, 0.0034%)</td>\n",
       "      <td>(movi start, 0.0036%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>(camera work, 0.0034%)</td>\n",
       "      <td>(first half, 0.0036%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>(pleasant surpris, 0.0034%)</td>\n",
       "      <td>(everyth els, 0.0036%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>(unit state, 0.0034%)</td>\n",
       "      <td>(writer director, 0.0036%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>(great act, 0.0033%)</td>\n",
       "      <td>(lead charact, 0.0035%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>(motion pictur, 0.0032%)</td>\n",
       "      <td>(best thing, 0.0035%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>(good guy, 0.0032%)</td>\n",
       "      <td>(few minut, 0.0034%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>(academi award, 0.0032%)</td>\n",
       "      <td>(young woman, 0.0034%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>(first movi, 0.0032%)</td>\n",
       "      <td>(last night, 0.0034%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>(civil war, 0.0032%)</td>\n",
       "      <td>(love stori, 0.0033%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>(other peopl, 0.0032%)</td>\n",
       "      <td>(scienc fiction, 0.0033%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>(young boy, 0.0031%)</td>\n",
       "      <td>(other peopl, 0.0033%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>(open scene, 0.0031%)</td>\n",
       "      <td>(plot twist, 0.0033%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>(sever time, 0.0031%)</td>\n",
       "      <td>(horror flick, 0.0032%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>(littl girl, 0.0031%)</td>\n",
       "      <td>(old man, 0.0032%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>(support role, 0.0031%)</td>\n",
       "      <td>(realli realli, 0.0032%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>(charact actor, 0.0031%)</td>\n",
       "      <td>(good stori, 0.0032%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>(happi end, 0.0031%)</td>\n",
       "      <td>(next time, 0.0031%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>(star trek, 0.003%)</td>\n",
       "      <td>(action sequenc, 0.0031%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>(good act, 0.003%)</td>\n",
       "      <td>(half hour, 0.0031%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>(big fan, 0.003%)</td>\n",
       "      <td>(complet wast, 0.0031%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>(film maker, 0.003%)</td>\n",
       "      <td>(other actor, 0.0031%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>(sex scene, 0.0029%)</td>\n",
       "      <td>(soap opera, 0.003%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>(good stori, 0.0029%)</td>\n",
       "      <td>(subject matter, 0.003%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(second time, 0.0029%)</td>\n",
       "      <td>(same thing, 0.0029%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(york citi, 0.0029%)</td>\n",
       "      <td>(littl girl, 0.0029%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(new york citi, 0.0029%)</td>\n",
       "      <td>(lead actor, 0.0029%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(lead role, 0.0029%)</td>\n",
       "      <td>(good look, 0.0029%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       positive                    negative\n",
       "0          (first time, 0.016%)    (special effect, 0.025%)\n",
       "1            (new york, 0.015%)        (worst movi, 0.019%)\n",
       "2        (main charact, 0.014%)      (main charact, 0.018%)\n",
       "3      (special effect, 0.014%)          (bad movi, 0.016%)\n",
       "4           (same time, 0.013%)       (horror movi, 0.016%)\n",
       "5      (high recommend, 0.011%)        (low budget, 0.015%)\n",
       "6          (great movi, 0.011%)       (horror film, 0.013%)\n",
       "7           (good movi, 0.011%)           (bad guy, 0.013%)\n",
       "8          (horror film, 0.01%)        (only thing, 0.012%)\n",
       "9            (real life, 0.01%)         (good movi, 0.012%)\n",
       "10        (great film, 0.0096%)           (bad act, 0.012%)\n",
       "11       (high school, 0.0093%)         (worst film, 0.01%)\n",
       "12         (long time, 0.0081%)      (high school, 0.0098%)\n",
       "13       (horror movi, 0.0078%)       (good thing, 0.0094%)\n",
       "14           (bad guy, 0.0075%)       (whole movi, 0.0094%)\n",
       "15        (stori line, 0.0075%)      (whole thing, 0.0086%)\n",
       "16        (other film, 0.0073%)       (stori line, 0.0082%)\n",
       "17        (love stori, 0.0072%)          (new york, 0.008%)\n",
       "18          (good job, 0.0072%)      (only reason, 0.0076%)\n",
       "19        (everi time, 0.0071%)       (first time, 0.0073%)\n",
       "20          (great job, 0.007%)        (sex scene, 0.0069%)\n",
       "21        (low budget, 0.0069%)         (bad film, 0.0067%)\n",
       "22         (best film, 0.0067%)     (product valu, 0.0066%)\n",
       "23         (good film, 0.0067%)        (real life, 0.0064%)\n",
       "24       (worth watch, 0.0064%)        (same time, 0.0063%)\n",
       "25         (world war, 0.0064%)        (long time, 0.0061%)\n",
       "26         (young man, 0.0063%)        (other movi, 0.006%)\n",
       "27         (best movi, 0.0062%)       (everi time, 0.0058%)\n",
       "28         (mani time, 0.0062%)        (mani time, 0.0057%)\n",
       "29        (mani peopl, 0.0062%)      (fight scene, 0.0057%)\n",
       "..                          ...                         ...\n",
       "70       (action movi, 0.0035%)       (someth els, 0.0037%)\n",
       "71        (last night, 0.0034%)       (video game, 0.0037%)\n",
       "72      (favorit movi, 0.0034%)       (movi start, 0.0036%)\n",
       "73       (camera work, 0.0034%)       (first half, 0.0036%)\n",
       "74  (pleasant surpris, 0.0034%)      (everyth els, 0.0036%)\n",
       "75        (unit state, 0.0034%)  (writer director, 0.0036%)\n",
       "76         (great act, 0.0033%)     (lead charact, 0.0035%)\n",
       "77     (motion pictur, 0.0032%)       (best thing, 0.0035%)\n",
       "78          (good guy, 0.0032%)        (few minut, 0.0034%)\n",
       "79     (academi award, 0.0032%)      (young woman, 0.0034%)\n",
       "80        (first movi, 0.0032%)       (last night, 0.0034%)\n",
       "81         (civil war, 0.0032%)       (love stori, 0.0033%)\n",
       "82       (other peopl, 0.0032%)   (scienc fiction, 0.0033%)\n",
       "83         (young boy, 0.0031%)      (other peopl, 0.0033%)\n",
       "84        (open scene, 0.0031%)       (plot twist, 0.0033%)\n",
       "85        (sever time, 0.0031%)     (horror flick, 0.0032%)\n",
       "86        (littl girl, 0.0031%)          (old man, 0.0032%)\n",
       "87      (support role, 0.0031%)    (realli realli, 0.0032%)\n",
       "88     (charact actor, 0.0031%)       (good stori, 0.0032%)\n",
       "89         (happi end, 0.0031%)        (next time, 0.0031%)\n",
       "90          (star trek, 0.003%)   (action sequenc, 0.0031%)\n",
       "91           (good act, 0.003%)        (half hour, 0.0031%)\n",
       "92            (big fan, 0.003%)     (complet wast, 0.0031%)\n",
       "93         (film maker, 0.003%)      (other actor, 0.0031%)\n",
       "94         (sex scene, 0.0029%)        (soap opera, 0.003%)\n",
       "95        (good stori, 0.0029%)    (subject matter, 0.003%)\n",
       "96       (second time, 0.0029%)       (same thing, 0.0029%)\n",
       "97         (york citi, 0.0029%)       (littl girl, 0.0029%)\n",
       "98     (new york citi, 0.0029%)       (lead actor, 0.0029%)\n",
       "99         (lead role, 0.0029%)        (good look, 0.0029%)\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(review_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fae329ae254b7aa17c577678b44562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56ccaddd89a422c9723d5d59c6eba52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_frequent_words_with_single = {}\n",
    "for sentiment, review in reviews.items():\n",
    "    review_frequent_words_with_single[sentiment] = extract_frequent_words(review, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termes plus fréquents (incluant les termes singuliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(film, 0.86%)</td>\n",
       "      <td>(movi, 0.96%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(movi, 0.73%)</td>\n",
       "      <td>(film, 0.77%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(time, 0.27%)</td>\n",
       "      <td>(bad, 0.27%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(good, 0.25%)</td>\n",
       "      <td>(time, 0.26%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(charact, 0.23%)</td>\n",
       "      <td>(good, 0.25%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(great, 0.23%)</td>\n",
       "      <td>(charact, 0.24%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(stori, 0.23%)</td>\n",
       "      <td>(scene, 0.19%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(other, 0.19%)</td>\n",
       "      <td>(stori, 0.18%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(scene, 0.17%)</td>\n",
       "      <td>(other, 0.17%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(way, 0.15%)</td>\n",
       "      <td>(thing, 0.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(more, 0.14%)</td>\n",
       "      <td>(act, 0.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(peopl, 0.14%)</td>\n",
       "      <td>(peopl, 0.15%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(best, 0.13%)</td>\n",
       "      <td>(plot, 0.15%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(life, 0.13%)</td>\n",
       "      <td>(way, 0.14%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(year, 0.13%)</td>\n",
       "      <td>(end, 0.13%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(end, 0.13%)</td>\n",
       "      <td>(more, 0.13%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(show, 0.12%)</td>\n",
       "      <td>(actor, 0.12%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(thing, 0.12%)</td>\n",
       "      <td>(realli, 0.12%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(first, 0.12%)</td>\n",
       "      <td>(first, 0.11%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(man, 0.11%)</td>\n",
       "      <td>(much, 0.11%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(mani, 0.11%)</td>\n",
       "      <td>(actual, 0.1%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(perform, 0.11%)</td>\n",
       "      <td>(watch, 0.1%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(actor, 0.11%)</td>\n",
       "      <td>(look, 0.1%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(act, 0.1%)</td>\n",
       "      <td>(show, 0.099%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(littl, 0.1%)</td>\n",
       "      <td>(guy, 0.097%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(work, 0.1%)</td>\n",
       "      <td>(great, 0.093%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(love, 0.095%)</td>\n",
       "      <td>(littl, 0.093%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(realli, 0.094%)</td>\n",
       "      <td>(director, 0.092%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(much, 0.094%)</td>\n",
       "      <td>(dont, 0.089%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(part, 0.093%)</td>\n",
       "      <td>(mani, 0.089%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>(final, 0.056%)</td>\n",
       "      <td>(kind, 0.057%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>(last, 0.056%)</td>\n",
       "      <td>(idea, 0.057%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>(perfect, 0.056%)</td>\n",
       "      <td>(cast, 0.056%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>(effect, 0.056%)</td>\n",
       "      <td>(day, 0.056%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>(point, 0.055%)</td>\n",
       "      <td>(music, 0.056%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>(war, 0.055%)</td>\n",
       "      <td>(role, 0.056%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>(direct, 0.053%)</td>\n",
       "      <td>(perform, 0.055%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>(wonder, 0.053%)</td>\n",
       "      <td>(stupid, 0.055%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>(better, 0.053%)</td>\n",
       "      <td>(play, 0.054%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>(horror, 0.052%)</td>\n",
       "      <td>(money, 0.054%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>(dont, 0.052%)</td>\n",
       "      <td>(pretti, 0.054%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>(ive, 0.052%)</td>\n",
       "      <td>(fan, 0.054%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>(moment, 0.052%)</td>\n",
       "      <td>(book, 0.054%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>(job, 0.051%)</td>\n",
       "      <td>(comedi, 0.053%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>(place, 0.05%)</td>\n",
       "      <td>(complet, 0.052%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>(someth, 0.05%)</td>\n",
       "      <td>(name, 0.051%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>(origin, 0.05%)</td>\n",
       "      <td>(last, 0.051%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>(fun, 0.05%)</td>\n",
       "      <td>(person, 0.051%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>(tri, 0.049%)</td>\n",
       "      <td>(friend, 0.05%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>(excel, 0.049%)</td>\n",
       "      <td>(kid, 0.05%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>(differ, 0.048%)</td>\n",
       "      <td>(main, 0.05%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>(line, 0.048%)</td>\n",
       "      <td>(bit, 0.049%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>(use, 0.048%)</td>\n",
       "      <td>(sure, 0.049%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>(live, 0.048%)</td>\n",
       "      <td>(final, 0.048%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>(episod, 0.048%)</td>\n",
       "      <td>(world, 0.048%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>(true, 0.048%)</td>\n",
       "      <td>(use, 0.047%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(kind, 0.047%)</td>\n",
       "      <td>(love, 0.047%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(american, 0.047%)</td>\n",
       "      <td>(place, 0.047%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(woman, 0.047%)</td>\n",
       "      <td>(hard, 0.047%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(dvd, 0.047%)</td>\n",
       "      <td>(obvious, 0.047%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              positive            negative\n",
       "0        (film, 0.86%)       (movi, 0.96%)\n",
       "1        (movi, 0.73%)       (film, 0.77%)\n",
       "2        (time, 0.27%)        (bad, 0.27%)\n",
       "3        (good, 0.25%)       (time, 0.26%)\n",
       "4     (charact, 0.23%)       (good, 0.25%)\n",
       "5       (great, 0.23%)    (charact, 0.24%)\n",
       "6       (stori, 0.23%)      (scene, 0.19%)\n",
       "7       (other, 0.19%)      (stori, 0.18%)\n",
       "8       (scene, 0.17%)      (other, 0.17%)\n",
       "9         (way, 0.15%)      (thing, 0.16%)\n",
       "10       (more, 0.14%)        (act, 0.16%)\n",
       "11      (peopl, 0.14%)      (peopl, 0.15%)\n",
       "12       (best, 0.13%)       (plot, 0.15%)\n",
       "13       (life, 0.13%)        (way, 0.14%)\n",
       "14       (year, 0.13%)        (end, 0.13%)\n",
       "15        (end, 0.13%)       (more, 0.13%)\n",
       "16       (show, 0.12%)      (actor, 0.12%)\n",
       "17      (thing, 0.12%)     (realli, 0.12%)\n",
       "18      (first, 0.12%)      (first, 0.11%)\n",
       "19        (man, 0.11%)       (much, 0.11%)\n",
       "20       (mani, 0.11%)      (actual, 0.1%)\n",
       "21    (perform, 0.11%)       (watch, 0.1%)\n",
       "22      (actor, 0.11%)        (look, 0.1%)\n",
       "23         (act, 0.1%)      (show, 0.099%)\n",
       "24       (littl, 0.1%)       (guy, 0.097%)\n",
       "25        (work, 0.1%)     (great, 0.093%)\n",
       "26      (love, 0.095%)     (littl, 0.093%)\n",
       "27    (realli, 0.094%)  (director, 0.092%)\n",
       "28      (much, 0.094%)      (dont, 0.089%)\n",
       "29      (part, 0.093%)      (mani, 0.089%)\n",
       "..                 ...                 ...\n",
       "70     (final, 0.056%)      (kind, 0.057%)\n",
       "71      (last, 0.056%)      (idea, 0.057%)\n",
       "72   (perfect, 0.056%)      (cast, 0.056%)\n",
       "73    (effect, 0.056%)       (day, 0.056%)\n",
       "74     (point, 0.055%)     (music, 0.056%)\n",
       "75       (war, 0.055%)      (role, 0.056%)\n",
       "76    (direct, 0.053%)   (perform, 0.055%)\n",
       "77    (wonder, 0.053%)    (stupid, 0.055%)\n",
       "78    (better, 0.053%)      (play, 0.054%)\n",
       "79    (horror, 0.052%)     (money, 0.054%)\n",
       "80      (dont, 0.052%)    (pretti, 0.054%)\n",
       "81       (ive, 0.052%)       (fan, 0.054%)\n",
       "82    (moment, 0.052%)      (book, 0.054%)\n",
       "83       (job, 0.051%)    (comedi, 0.053%)\n",
       "84      (place, 0.05%)   (complet, 0.052%)\n",
       "85     (someth, 0.05%)      (name, 0.051%)\n",
       "86     (origin, 0.05%)      (last, 0.051%)\n",
       "87        (fun, 0.05%)    (person, 0.051%)\n",
       "88       (tri, 0.049%)     (friend, 0.05%)\n",
       "89     (excel, 0.049%)        (kid, 0.05%)\n",
       "90    (differ, 0.048%)       (main, 0.05%)\n",
       "91      (line, 0.048%)       (bit, 0.049%)\n",
       "92       (use, 0.048%)      (sure, 0.049%)\n",
       "93      (live, 0.048%)     (final, 0.048%)\n",
       "94    (episod, 0.048%)     (world, 0.048%)\n",
       "95      (true, 0.048%)       (use, 0.047%)\n",
       "96      (kind, 0.047%)      (love, 0.047%)\n",
       "97  (american, 0.047%)     (place, 0.047%)\n",
       "98     (woman, 0.047%)      (hard, 0.047%)\n",
       "99       (dvd, 0.047%)   (obvious, 0.047%)\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(review_frequent_words_with_single)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
